Number,Text
1,the provision of end-users’ feedback
2,"numerous reviews 4, which are open to public access"
3,"significantly assists app developers in their end-users’ driven software quality evaluations, product marketing, and software maintenance"
4,manually extracting useful reviews from vast volumes of reviews
5,"high levels of cognitive load, effort, and time. The"
6,non-useful reviews present in the app reviews repository of
7,and so on. The filtering of non- useful reviews
8,Naïve Bayes method stands out as one of the most suitable for
9,we have not observed published efforts aimed at
10,related to the extraction of useful reviews. Section 3 describes the
11,and concepts that assisted us in formulating the variants of Naïve Bayes. The
12,https://play.google.com/store 2 https://www.apple.com/nz/ios/app-store
13,to perform filtering based on disambiguation (contextual mean- ing) of the
14,"For instance, Keertipati et al. 9 have extracted app features"
15,Fu et al. 10 have performed sentiment analysis using logistic regression to
16,with the assumption that negative reviews reflect severe app issues
17,"In another study, Shah et al. 12 have evaluated the"
18,"requests, bugs or suggestions associated with the app features 16 (e.g"
19,Such approach classifies app reviews having common attributes into specific categories
20,"based on a taxonomy derived manually from domain knowledge, as a review of the literature shows that all the classification methods for classifying app reviews are dependent on domain knowledge made available manually through means of extensive research or by domain experts. For instance, Panichella et al. 18 have inherited a taxonomy from the taxonomy proposed by Pagano et al. 3 and have evaluated the classification performance SVM (Support Vector Machines"
21,"Naïve Bayes, Decision Tress and Logistic Regression. Pagano et al. 3 have manually as- signed categories that constitute a taxonomy for classifying app reviews. Similarly, Maalej et al. 4 manually developed four categories to classify app reviews using methods such as keyword lookup classifying mechanism, Decision Tress, Naïve Bayes and Maximum En- tropy. Such studies have provided inspiration for others. For instance, Panichella et al. 19 developed a manual taxonomy that was inherited from the taxonomy created by Panichella et al. 18 to automatically classify reviews using the J48 supervised machine learning method. In another study, Ciurumelea et al. 20 have come up with five sets of categories by taking inspiration from"
22,and created a taxonomy to classify reviews using Gradient Boosting supervised machine learning method. Similarly
23,Sorbo et al. 21 have developed a fine-grained taxonomy from the taxonomy proposed by Panichella et al. 18 which consists of additional categories over the study it is based on
24,the classification methods that classified reviews of apps we
25,"This drawback being, that all the classification methods were driven by manually derived taxonomy which is problematic when the domain knowledge is"
26,Another drawback of utilizing a manually created taxonomy
27,to update the domain knowledge to create a new version of the taxonomy when the app evolves and new reviews are logged by the end-users
28,"reviews that reflect feature requests, bugs or"
29,a set of linguistic rules to extract app feature requests
30,Huang et al. 26 have developed a
31,from a training set of manually pre-labelled
32,"appropriate labels (i.e., availability, look and"
33,"In a recent study, Panichella et. al 27 have"
34,a large amount of pre-classified learning data
35,"That said, Multinomial Naïve Bayes is"
36,supervised machine learning method that has been empirically evaluated to be a suitable choice for text related software engineering applications
37,"due to its handling of generalization towards predictions on new data, further leading towards the requirement of less training data for learning"
38,Multinomial Naïve Bayes 32. This method has been
39,widely used in software engineering applications such as
40,"were not provided. In addition, even though Naïve Bayes was utilized to filter useful reviews belonging to four apps"
41,the performance of Naïve Bayes variants for their utility towards extracting useful app reviews
42,raises the question: Under which circumstances does Naïve Bayes method deliver the best performance
43,Naïve Bayes methods and concepts that are specialized in
44,variants of the Naïve Bayes method. The
45,is to assist app developers in
46,useful reviews for software maintenance and evolution
47,"What are the performances of Naïve Bayes variants when extracting useful reviews? RQ2. Are there differences in outcomes for different Naïve Bayes implementations, and par- ticularly when considering data imbalances? The"
48,"As app developers need to address useful reviews in a timely manner, time"
49,"different experimental settings (i.e., research methodology, data for experimentation, validation procedures and"
50,"3. Methods and Concepts In this section, we introduce the"
51,and concepts which assisted us in generat- ing the respective variants
52,The prime objective of the variants is to auto- matically filter (via classification) useful and non-useful reviews
53,set of useful and non-useful
54,can be manually identified using a pre-defined set of rules for filtering proposed
55,"to useful reviews reflect feature requests (e.g., ‘please add feature"
56,"e.g., ‘I suggest you increase the font size for"
57,"non-useful reviews contain irrelevant and unwanted information (e.g., ‘this app is useless, uninstalling asap!’). Once the particular variant of"
58,distinguish useful reviews from non-useful
59,the objective of the respective Naïve Bayes variant is to assign
60,"to one of the two defined categories (useful and non-useful reviews, wherein each category is expected to contain reviews with properties reflecting the filter- ing rules. In the learning (training) phase, the particular Naïve Bayes variant"
61,that predicts the categories of new reviews in the classification (pre- diction/testing) phase. In the
62,followed by the concepts of Laplace Smoothing and Expectation
63,"by removing whitespaces, numbers, special characters (e.g., $, #) and punctuations (e.g"
64,"stop words (e.g., is, and"
65,to generate the complete dictionary form of words present in the pre-processed reviews
66,steps are standard text preprocessing procedures that are followed by researchers to
67,the generation of unreliable noisy results
68,that provides the necessary word frequency information for the Naïve Bayes variants
69,3.2 Multinomial Naïve Bayes Multinomial Naïve Bayes is a customized version of the basic Naïve Bayes method which is specialized for text classification
70,"on the principle of maxi- mum likelihood estimates. That is, it uses"
71,of the Multinomial Naïve Bayes is
72,"the probability of a review belonging to a particular category (cn) which is given as: P(cn) = Nreviews(r=cn)/Nreviews (1) Where, Nreviews indicates the number of reviews"
73,indicates the number of reviews belonging to a category
74,The maximum likelihood estimation is given as: P
75,the conditional probability of the word wi given
76,"is given as the ratio of the total number of times a word wi occurs in category cn to the total number of words w in the reviews of category cn. That is, the"
77,"of times word wi appears among all words (V) in the reviews of category cn. Thus, the Multinomial Naïve Bayes creates a word space for category cn by creating a dictionary of words belonging to the reviews of category"
78,"the frequency of each word w. Finally, using equations (1) and (2), the category of a review R can be determined using: CMAP (R) = argmaxcn (P(cn) * ?i P(wi"
79,From the manually categorized pre-processed app reviews
80,terms 2.1 For each cn in C do: 2.1.1 reviewsn ? all reviews with category = cn 2.1.2 P(cn) ?
81,3.1 Calculate P(wi
82,"In this sub-section, we discuss"
83,Naïve Bayes. Complement Naïve Bayes addresses the inability of Multinomial Naïve Bayes
84,the likelihood of a category of cn using training data of the other
85,"Complement Naïve Bayes, the prior probability"
86,"Multinomial Naive Bayes, Complement Naive Bayes calculates the likelihood of a word wi by considering its occurrences in category(ies) cn? other than"
87,probability of word wi given it belongs to category(ies) cn? is given as the ratio of the total number of times a word wi occurs in category(ies) cn? to the total number of words w in the reviews of category
88,"the Complement Naïve Bayes creates a word space for category cn by creating a dictionary of words belonging to the reviews of category(ies) cn? by utilizing the frequency of w. Finally, using equations (1) and (4), the category of a review R can be determined using: CMAP (R) = argmincn (P(cn) * ?i (1/ (P(wi"
89,given as the argument of the minimum of likelihood estimates of the category computed as priori times the inverse likelihood
90,From the manually categorized pre-processed app reviews
91,terms 2.1 For each cn in C do: 2.1.1 reviewsn ? all reviews with category = cn 2.1.2 P(cn) ?
92,3.1 Calculate P(wi
93,3.4 Laplace Smoothing From equations (2) and (4) it is evident that the
94,which in turn affects the accuracy of classification. This drawback is addressed by subjecting the parameters to Laplace Smoothing
95,Naïve Bayes method to keep track of the count of words in determining the relevant category
96,"is of prime importance especially when the particular Naïve Bayes method encounters a word in the classification phase (prediction/testing) whose information was not made available in the learning (training) phase. Hence, we"
97,information related to missing word wi. For the Multinomial Naïve Bayes
98,"parameter that performs maximum like- lihood estimation based on Laplace smoothing, given as: P"
99,"Similarly, for Complement Naïve Bayes, using equation (4"
100,that performs maximum likelihood estimation based on Laplace smoothing given as: P
101,"in the numerator, the size of the vocabulary ("
102,"sub-section 3.2), and Complement Naïve Bayes"
103,require a substantial number of manually categorized reviews to learn a classifier that is capable of accurately predicting the category of a new review
104,of reviews might become a time-consuming task associated with potential errors
105,by reducing the labelling effort demanded from
106,"of two steps, Expectation (E) and Maximization (M). The Ex- pectation step predicts and generates the absent information based on the current maxi- mum likelihood estimation parameters initiated by the method"
107,"Multinomial Naïve Bayes), while the Maximization step iteratively re-estimates the parameters thus maximizing the overall likelihood 48. Hence, EM allows the Multinomial Naïve Bayes method to run repeatedly until the"
108,the Multinomial Naïve Bayes method men- tioned in sub-sections 3.2
109,EM concept for this study was developed according to the
110,"of EM would comprise of training the Multinomial Naïve Bayes method on known categories of reviews, and then later, using the learned information on categories associated with the reviews to make predictions on the uncategorized reviews. Hence, these predictions can later be transformed into catego- ries, and therefore, can be utilized for subsequent training of the Multinomial Naïve Bayes method using the uncategorized reviews with the previously generated categories. Finally, the entire procedure is repeated until the value of the"
111,becomes constant (likelihood is computed using the entire corpus of app reviews). The detailed elaboration of the
112,reviews wherein each review R is tagged with a category C (useful or non-useful). The prime objective of EM is to generate the categories of the uncategorized reviews based on the Multinomial Naïve Bayes
113,"In every cycle, EM calculates the relevant probabilistic category and assigns it to the particular uncategorized review, that is P(cn"
114,Begin 1. Train the Multinomial Naïve Bayes method mNB from the manually
115,"R. 2. Expectation (E): 2.1 For each review Ri in the review set AR 2.1.1 Using the method mNB, calculate P(cn"
116,"Complement Naïve Bayes method does not support any generative interpre- tations, thus the creation of its EM variant is not possible"
117,Naïve Bayes variants as an outcome of the
118,variants. The prime objective in formulating these variants is to
119,reviews pertaining to an app. To
120,"variants belonging to the Multinomial Naïve Bayes method. Based on the method mentioned in sub-section 3.2, and the concepts mentioned in sub"
121,related to the Mul- tinomial Naïve Bayes method. We introduce the
122,of the Multinomial Naïve Bayes method mentioned in sub-section 3.2
123,allows the Multinomial Naïve Bayes method to
124,Laplace Smoothing with the Multinomial Naïve Bayes method
125,"semi-supervised version of III, and a post version of II"
126,"the variants related to the Complement Naïve Bayes method. Based on the method mentioned in sub-section 3.3, we generate the"
127,"of the Complement Naïve Bayes method. Next, based on sub-sections 3"
128,a post version of V. Table 1. Naïve Bayes
129,"Variant Name Description I Multinomial Naïve Bayes This variant is the Multinomial Naïve Bayes method described in sub-section 3.2. II Expectation Maximization - Multinomial Naïve Bayes The Expectation Maximization concept de- scribed in sub-section 3.5 has been incorpo- rated in I. Thus, this variant is the semi-super- vised version of I"
130,Multinomial Naïve Bayes with Laplace smoothing The Multinomial Naïve Bayes method has been incorporated with the concept of Laplace smoothing
131,this variant is the post version of I. IV Expectation Maximization - Multinomial Naïve Bayes with Laplace smoothing The Multinomial Naïve Bayes method has been incorporated with the concept of Laplace smoothing
132,"semi-supervised version of III, and post version of II. V Complement Naïve Bayes This variant is the Complement"
133,Naïve Bayes method described in sub-section 3.3. VI Complement Naïve Bayes with Laplace smoothing
134,"has been incorporated with the concept of Laplace smoothing. Thus, this variant is the post ver- sion of V. 4. Experimental Setting In this study, the Naïve Bayes variants described in Table 1 were implemented using the"
135,"suitable libraries provided by the Natural Lan- guage Tool Kit (NLTK)4, numpy5 and the scikit-learn6 packages"
136,evaluation of all six Naïve Bayes variants using datasets consisting of app reviews
137,"dataset consisted of 4003 reviews, VodafoneNZ consisted of"
138,consisted of 3683 reviews and Flutter dataset consisted of 3483 reviews. Using the set of rules defined in
139,Reviews as Useful or Non-Useful
140,https://www.python.org/ 4 https://www.nltk.org/ 5 https://numpy.org/ 6 https://scikit-learn.org
141,the recommended validation practices of the software engineering discipline
142,was undertaken to empirically evaluate the performance of six
143,results generated from human decisions with the results generated by the respective Naïve Bayes variant
144,Based on the manual labelling task
145,dataset consisted of 1154 (25%) useful reviews and 3405 (75%) non-useful reviews
146,dataset consisted of 1638 (41%) useful reviews and 2365 (59%) non-useful reviews
147,useful reviews and 5463 (83%) non-useful reviews
148,"Flutter dataset consisted of 2433 (70%) useful reviews and 1063 (30%) non-useful reviews, making it imbalanced"
149,reviews were independently labelled useful or non-useful by the
150,Fleiss’ Kappa which is the extended version of Cohen’s Kappa to support the evaluations of three or more human evaluators 55. The Fleiss
151,"coefficients were found to be 0.68 (substantial agreement), 0.74 (substantial agreement), 0.71 (substantial agreement"
152,"0.65 (substantial agreement), and 0.78 (substantial agreement) for"
153,Follow up discussions were held among the authors to resolve any disagreements and establish consensus for achiev- ing a reliable manual labelling process
154,using the particular Naïve Bayes variant is to correctly identify the type of each review
155,F-Measure and time metrics. Accu- racy as
156,given as the number of correctly classified reviews among the total number of classified reviews
157,given as the correctly classified useful reviews to the total number of reviews
158,"true positives and false negatives 38. Finally, F-Measure is computed as the harmonic mean of precision and recall, which validates robustness of the variants"
159,from a set of manually categorized reviews
160,"a CORE i5 CPU. For each experiment, we randomly split the respective dataset into a training set (90%) that is used to learn the relevant Naïve Bayes variant for reviews, and a testing set (10%), which is used to evaluate their performance in"
161,nondisclosed reviews. Every experiment was run 100 times using ten-fold cross
162,to obtain average scores for the metrics mentioned above
163,is traditionally followed by researchers to validate the stability of the methods
164,What are the performances of Naïve Bayes variants when extracting useful reviews
165,We present the results of the experiments conducted on the five datasets in
166,we report the average results of 100 times ten-fold cross-validation operations conducted on
167,based on the metrics mentioned in Section 4
168,the Shapiro-Wilk test to check the distribution of the results generated by each Naïve Bayes variant
169,"p-value<0.01). Thus, we ran the Krus- kal-Wallis non-parametric test to identify potential statistically significant differences be- tween the results of the Naïve Bayes variants"
170,we performed pairwise Wilcox testing to evaluate pairwise
171,for all comparisons (p-value<0.01
172,Variant Accuracy (%) Precision (0-1) Recall (0-1) F (0-1) (seconds) Time
173,varied performances exhibited by the Naive Bayes variants
174,the six Naïve Bayes variants on the TradeMe dataset
175,variant I had the lowest accuracy (59.3%) and F-Measure (0.57) when compared to
176,"exhibited the highest accuracy (80.2%) and F-Measure (0.65). Variant VI also required the least amount of time for learn- ing and prediction purposes (0.10 seconds), while variant II required the most time (0.29 seconds). Next, we tested the six variants"
177,variant I had the lowest accuracy (68.1%) and F-Meas- ure (0.71) compared to
178,"variant IV exhibited the highest accuracy (89.2%) and F-Measure (0.89). That said, variant VI required the least time for learning and prediction purposes (0.10 seconds), while variant II required most time (0.30 seconds). Similarly, we tested the six variants"
179,variant I had the lowest accuracy and F-Measure
180,the highest accuracy and F-Measure
181,"variant II had the highest time requirement (0.40 seconds), and"
182,variants II and V exhibit very large differences in magnitude for accuracy
183,variant I had the lowest accuracy (60.2%) and F-Measure (0.72) compared to
184,"variant IV exhibited the highest accuracy (78.2%) and F- Measure (0.81). That said, variant VI had the least time"
185,"seconds), while variant II required more time"
186,the six Naïve Bayes variants on the Flutter dataset
187,"variant I had the lowest accuracy (76.2%), while VI"
188,F-Measure (0.89) with the least time (0.08 seconds
189,"variant II had the highest time requirement (0.23 seconds), and variant IV"
190,accuracy (82.3%) and F-Measure (0.88
191,"variants II, III, and V did not exhibit very large differences in magnitude for accuracy and F-Measure (notwithstanding these differences were statistically significant p-value<0.01"
192,to correctly classify useful reviews (from all app reviews
193,correlation test to investigate the association between the scale of data imbalance and the accuracy
194,Multinomial Naïve Bayes with Laplace smoothing) and V (Complement Naïve Bayes
195,Multinomial Naïve Bayes with Laplace smoothing) and VI (Complement Naïve Bayes with Laplace smoothing
196,Complement Naïve Bayes incorporated with Laplace Smoothing) is
197,the expectation maximization variants (II and IV
198,in comparison to their predecessors (I and II
199,"in comparison to their previous versions (III-I, IV-II and VI-V"
200,the expectation maximization variants (II and IV) required more time
201,the time required for learning and prediction purposes
202,statistically significant (p-value<0.01
203,"Accuracy, F-Measure and Time of"
204,"p-value <0.01) Finally, we conducted the"
205,test to investigate the association between the results
206,What are the performances of Naïve Bayes variants when extracting useful reviews
207,"provides a summary of performance results (accuracy, F-Measure and time metrics) of the six Naïve Bayes variants for the five datasets"
208,"based on the results conveyed through the accuracy, F-Measure and time metrics"
209,a significant role in predicting the relevant label
210,"variations in performances exhibited for the Naïve Bayes variants when classifying useful and non-useful reviews for the five datasets. Based on this outcome, we believe that the variants may reliably predict the label associated with each review if the features"
211,"degree of distinctness (i.e., if the features associated with a label are significantly discrete in comparison to the features associated with the other labels), an aspect that requires further empirical"
212,"Overall performance of Naïve Bayes variants based on accuracy, F-Measure and time"
213,"that all the Naïve Bayes variants operated on the as- sumption of independence, which causes each variant to disregard the meaning of words it processed relative to other words. This"
214,"For example, in the review ‘the signal"
215,are related as the word pair ‘signal - fades’ indicates that
216,"That said, other machine learning algorithms such as logistic regression"
217,attempt to fit a normal curve
218,variant assumes that the word space is normally distributed with zero variance between words in all categories. This
219,in some cases the particular variant may be unable to generate a reliable discretization of interrelated (continuous) word features. This may potentially compromise
220,would be to test for the independence of the words to get a tentative estimate of prediction errors to determine the suitability of
221,generate a zero normal distribution towards producing more efficient results
222,"89% accuracy, 0.87 precision, 0.98 recall, 0.89 F-meas- ure, and 0.08 seconds time). Thus, the Naïve Bayes variants"
223,"investigated in this work, on their own, hold promise for aiding useful reviews filtering to support software"
224,Are there differences in outcomes for different Naïve Bayes implementations
225,variants (II and IV) significantly improved the
226,Multinomial Naïve Bayes variants (I and III). The Expectation
227,Multinomial Naïve Bayes and Expectation Maximization-Multinomial Naïve Bayes with Laplace smoothing
228,Multinomial Naïve Bayes and Multinomial Naïve Bayes with Laplace smoothing
229,Multinomial Naïve Bayes and Expectation Maxi- mization-Multinomial Naïve Bayes with Laplace smoothing
230,increase in time). The increase in accuracy and F-Measure noted in Section 5 is due to the working mechanism of Expectation Maximization that allows
231,to gain maximum information about the underlying words present in reviews be- longing to the same category
232,"3.5 when uncategorized and categorized reviews are passed to the Expectation Maximization variant, which in turn allows the Expectation Maximization variant to gain insights on the different types of words pertaining to a particular category in its learning phase. The"
233,"accuracy and F-measure. That said, the operating structure of the Multinomial Naïve Bayes and Multi- nomial Naïve Bayes with Laplace smoothing work based on closed-form formulas 68, which allow these variants to generate results quickly"
234,"Multinomial Naïve Bayes and Expectation Maximization-Multinomial Na- ïve Bayes with Laplace smoothing, which generate results based on an iterative"
235,"likelihood parameters to become constant), thus requiring more time"
236,"Laplace smoothing, results show that this enhancement assisted signif- icantly in increasing the accuracy and F-measure, and reducing the time requirements for"
237,"involving Multinomial Naïve Bayes, Expectation Maximization-Multi- nomial Naïve Bayes and Complement Naïve Bayes"
238,"increase in accuracy, 0.15 improvement in F-Measure and 0.14 seconds reduction in time that were accounted for by the Laplace smoothing"
239,"enhanced the retrieval of useful reviews significantly. As observed from equations (6) and (7), Laplace smoothing avoids the zero counts of words whose information are not known in the training phase, thus preserving the value of maximum likelihood estimates that are crucial towards the computation of a category of review. Therefore, any maximum likelihood estimates being 0 causes a"
240,"determining the relevant category of a review. Subsequently, the variants"
241,"faster estimates of the parameters that compute the likelihood 69, hence improving Naïve Bayes prediction performance"
242,"Laplace smoothing assisted variants III, IV and VI"
243,"that, overall, Expectation Maxi- mization-Multinomial Naïve Bayes with Laplace smoothing performed well on the da- tasets in terms of accuracy and F-Measure. Thus, from a practical perspective, Expectation Maximization-Multinomial Naïve Bayes with Laplace smoothing (IV) may be a suita- ble candidate for the task"
244,Complement Naïve Bayes with Laplace smoothing (VI) performed well on the
245,allows it to perform well when the dataset
246,"datasets, Complement Naïve Bayes with Laplace smooth- ing (VI) had the least time requirements (average ~ 0.11 seconds). Hence, the application of Complement Naïve Bayes with Laplace smoothing is best suited when app develop- ers have a substantial amount of categorized reviews whose labels are imbalanced, and at the same time are bound by time constraints"
247,Complement Naïve Bayes with Laplace smoothing variant
248,variants of the Naïve Bayes method decreased as the
249,"by the Naïve Bayes method, F- Measures of the"
250,we have mitigated the threats related to labelling of app reviews by
251,"feedback provided by app developers, (2) studying and becoming famil- iar with the rules mentioned in Chen et al. 11 for labeling app reviews and, (3) rigorously analyzing various types of app reviews that app developers are concerned with. The rules pertaining to the"
252,"were discussed extensively among the authors for shared understanding, before reliability checks were conducted which returned sub- stantial agreements"
253,Follow up discussions were also held to establish consensus among the authors before finalizing the
254,"compare the performance of Naïve Bayes variants against each other for their effectiveness towards filtering of useful app reviews, addressing the"
255,the performance of other IR approaches or machine learning
256,"is not investigated in this work. However, potential future work aimed at conducting such an investigation could be planned. This investigation could involve the perfor- mance evaluation of popular machine learning algorithms such as BERT (Bidirectional Encoder Representations from Transformers), Decision Trees, Ran- dom Forests, Logistic Regression, SVM and so on, towards the filtering of useful reviews"
257,variants were beyond the scope of this study
258,"We used a computer with specific hardware configuration (refer to Section 4), which may limit the generalizability of"
259,"however, the pattern of results were con- sistent across the datasets and so this was not a threat to the pattern of outcomes observed"
260,Naïve Bayes variants to- wards filtering of useful reviews
261,"However, the main objective of this study was to examine the feasibility and performance of the variants towards filtering of useful reviews and quantifying the evaluation of the results generated by"
262,the time and human re- source constraints associated with the manual labelling of the reviews and
263,"Construct Validity To construct the ground truth to filter useful reviews we followed the well-estab- lished rules from a prominent study to label the app reviews 11. In addition, recommended practices from the software engineering discipline"
264,"consensus formation). However, another alternative to construct"
265,this ground truth would be to approach the app developers of the respective apps to ob- tain the labelled set of reviews for evaluating the performance of the filtering approach
266,"study, we investigated Naïve Bayes variants for their utility"
267,the approach incorporating Expectation Maximization for the Naïve Bayes
268,"the most promise. Thus, in this study, we investigate the performances of six variants of Naïve Bayes"
269,"Naïve Bayes with Laplace smoothing (i.e., variant IV) is best suited for extracting useful reviews"
270,"and Complement Naïve Bayes with Laplace smoothing (i.e., variant VI) may be best suited for extracting useful reviews"
271,Naïve Bayes variants for filtering of app reviews
272,variants of Multinomial Naïve Bayes method to validate the
273,"Beyond app reviews however, the utility of"
274,"software repositories such as Jira, GitHub and so on"
275,Datasets summary. App Name Total number of reviews logged Category Maximum review length (characters) Minimum review length (characters) length of Average review Average app rating Category
